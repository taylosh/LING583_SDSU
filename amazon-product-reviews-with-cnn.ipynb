{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.7"},"kaggle":{"accelerator":"gpu","dataSources":[],"dockerImageVersionId":29869,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import pandas as pd\nimport json, gzip\nfrom urllib.request import urlopen\nimport multiprocessing as mp\nfrom cytoolz import *\nfrom ftfy import fix_text\nimport re\nfrom sklearn.model_selection import *","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Do not forget to put your GPU option on (under Settings).","metadata":{}},{"cell_type":"markdown","source":"## Download and feature engineering","metadata":{}},{"cell_type":"code","source":"url = 'http://snap.stanford.edu/data/amazon/productGraph/categoryFiles/reviews_Patio_Lawn_and_Garden_5.json.gz'","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"data = gzip.decompress(urlopen(url).read())\ndata = data.splitlines()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(type(data))\ndata[4]","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df = pd.DataFrame(json.loads(line) for line in data)\ndf.head()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df.groupby('overall').size()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Get rid of four-star reviews (Dr. Malouf believed that they are too wishy-washy).","metadata":{}},{"cell_type":"code","source":"df = df[df['overall']!=4].copy()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df['sentiment'] = [1 if s>4 else 0 for s in df['overall']]","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"pd.value_counts(df['sentiment'])","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Cleaning text.\nFixing coding.\n\nLook for the method applied below here: https://ftfy.readthedocs.io/en/latest/","metadata":{}},{"cell_type":"code","source":"with mp.Pool() as p:\n    df['reviewText'] = p.map(fix_text, df['reviewText'])\n\ndf.reviewText.head()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"As you see I use for replacement 2 methods, one is basic Python `.replace` and another `re.sub`. I did it because the first one ignores regular expression and sometimes this allows simplier application.","metadata":{}},{"cell_type":"code","source":"def preproc_text(x):\n    x = str(x)\n    x = x.replace(\"'s \",' ')\n    x = re.sub(\"[\\t\\n\\r\\f\\v]\",' ', x)\n    x = x.replace('&', ' and ')\n    x = x.replace(' is ', \" \")\n    for punct in \"/-\":\n        x = x.replace(punct, ' ') # removing the symbols between words\n    x = re.sub('[\\W_]+', ' ', x) # keep only alphanumeric characters\n    x = re.sub(r'\\d \\d', '##', x) # to fix things like 5/8 or 2.5 which were left with a space instead of the slash or period.\n    x = re.sub('\\d', '#', x) # replace single digits with a '#' symbol\n    x = re.sub('\\d{2,}', '##', x) # replace double and more digits with '##'\n    x = re.sub('#{2,}', '##', x) # limiting a number of '#' sequential symbols to 2 \n    x = re.sub(r'([a-z](?=[A-Z])|[A-Z](?=[A-Z][a-z]))', r'\\1 ', x) # splitting stuck words like \"MowingSo\" or \"yearsORIGINAL\" or \"UPDATESIm\n    x = re.sub('(^[Aa]n )|( [Aa]n )|(^[Aa] )|( [Aa] )', ' ', x)\n    for key in misspell_dict:\n        x = re.sub(key, misspell_dict[key], x, flags=re.IGNORECASE)\n    x = x.strip() \n    x = re.sub(r' +', ' ', x)  # removing redundant spaces in the middle\n    return x","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"I found list of typical misspellings in a script which was shared with me. I modified it.","metadata":{}},{"cell_type":"code","source":"misspell_dict = {' aqm ': ' am ',\n                ' I m ': ' i am',\n                ' I d ': ' I would ',\n                'they ve ': 'they have ',\n                ' won t ': ' will not ',\n                ' ll ': ' will ', \n                ' can t ': ' can not ',\n                ' haven t': ' have not',\n                ' didn t':' did not',\n                ' aren t': ' are not',\n                ' doesn t':' does not',\n                ' hasn t':' has not',\n                ' wasn t':' was not',\n                ' couldn t': ' could not',\n                ' isn t': ' not',\n                'colour':'color',\n                'centre':'center',\n                'shouldn t':'should not',\n                'favourite':'favorite',\n                'travelling':'traveling',\n                'counselling':'counseling',\n                'theatre':'theater',\n                'cancelled':'canceled',\n                'labour':'labor',\n                'organisation':'organization',\n                'wwii':'world war',\n                'citicise':'criticize',\n                'instagram': 'social medium',\n                'whatsapp': 'social medium',\n                'snapchat': 'social medium',\n                'Snapchat': 'social medium',\n                'pinterest' : 'social medium',\n                'WeChat' : 'social medium',\n                'mny' : 'many',\n                'quora' : 'social medium',\n                'bitcoin' : 'dollar',\n                'cryptocurrency' : 'dollar',\n                'behaviour' : 'behavior',\n                'programme': 'program',\n                'realise':'realize',\n                'defence':'defense',\n                'cryptocurrencies' : 'currencies',\n                'Brexit':'brexit',\n                'honours':'honors',\n                'learnt':'learned',\n                'upvote':'like',\n                'licence':'license',\n                'Whatis':'what is',\n                'aluminium':'aluminum',\n                'favour':'favor',\n                'modelling':'modeling',\n                'recognise':'recognize',\n                'grey':'gray',\n                'programr':'programmer',\n                'travelled':'traveled',\n                'cheque':'check',\n                'judgement':'judgment',\n                'neighbour':'neighbor',\n                'analyse':'analyze',\n                'practise':'practice',\n                'litre':'liter'\n               }","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"with mp.Pool() as p:\n    df['reviewText'] = p.map(preproc_text, df['reviewText'])\n\ndf.reviewText[4]","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"wordN = max([len(doc.split(' ')) for doc in df[\"reviewText\"]])\nprint(wordN)\nlong_ref = [doc for doc in df[\"reviewText\"] if len(doc.split(' '))==wordN]\nlong_ref","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"It is long... Here is a phrase from its middle: \"I don t know how many words Amazon allows for reviews but I can see this will become blog\"\n\nThere was the following problem: some words are joined together, like \"MowingSo\" or \"yearsORIGINAL\" or \"UPDATESIm\".  I fixed it in my preprocessing method.","metadata":{}},{"cell_type":"markdown","source":"# Tokenization and Classification with `keras`","metadata":{"trusted":true}},{"cell_type":"code","source":"import keras\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing import sequence\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Conv1D, Flatten, Embedding,Dropout, MaxPooling1D, LSTM,  BatchNormalization\nfrom keras import regularizers","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"It turned out that `keras` module does not accept sparse matrices. There are ways to hande this problem. \n\n1. Introduce a method for each batch which convertes chosen rows into dense matrix.\n\n2. Go to `keras` backend, `tensorflow`, which has methods for sparse matrices.\n\n3. Reduce dimensions: \n  - using embeddings (the most popular on kaggle.com)\n  - chop off too frequent words with CountVectorizer, hoping that they are too ubiquitous to be significant for our classification\n  - use PCI or LSI. ","metadata":{}},{"cell_type":"code","source":"max_words =5000\nmax_sequence_length= wordN\ntokenizer = Tokenizer(num_words=max_words)\ntokenizer.fit_on_texts(df['reviewText'])\nsequences = tokenizer.texts_to_sequences(df['reviewText'])","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"type(sequences)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"sequences[:3]","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Now we can turn our sequences into a numpy array","metadata":{}},{"cell_type":"code","source":"data = sequence.pad_sequences(sequences, maxlen=max_sequence_length)\nprint(type(data))\ndata.shape","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"x_train,x_test, y_train, y_test = train_test_split(data,df['sentiment'],\n                             test_size=0.2,\n                             stratify=df['sentiment'],\n                             random_state=42)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"max_features = vocab_size = len(tokenizer.word_index) + 1\nmaxlen = max_sequence_length\nbatch_size = 750\nembedding_dims = 100\nfilters = 250\nkernel_size = 8\nhidden_dims = 64\nepochs = 50\nvocab_size","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"CNN_model = Sequential()\nCNN_model.add(Embedding(max_features,\n                    embedding_dims,\n                    input_length=maxlen))\n\n# we add a Convolution1D, which will learn filters\n# word group filters of size filter_length:\nCNN_model.add(Conv1D(filters,\n                 kernel_size,\n                 padding='valid',\n                 activation='relu',\n                 strides=1))\n\nCNN_model.add(Dropout(0.3))\nCNN_model.add(MaxPooling1D(pool_size=5))\n\n#Let us repeat a layer.\nCNN_model.add(Conv1D(filters,\n                 kernel_size,\n                 padding='valid',\n                 activation='relu',\n                 strides=1))\nCNN_model.add(MaxPooling1D(pool_size=5))\n\n#CNN_model.add(LSTM(64))\nCNN_model.add(Flatten())\nCNN_model.add(Dropout(0.2))\n\n# We add a simple hidden layer:\nCNN_model.add(Dense(hidden_dims, activation='relu', \n                    kernel_regularizer=regularizers.l1_l2(l1=0.1, l2=0.7)))\n\nCNN_model.add(Dropout(0.2))\n\n# We project onto a single unit output layer, and squash it with a sigmoid:\nCNN_model.add(Dense(1, activation='sigmoid'))\n\nCNN_model.compile(loss='binary_crossentropy',\n              optimizer='adam',\n              metrics=['accuracy'])\nhistory = CNN_model.fit(x_train, y_train,\n          batch_size=batch_size,\n          epochs=epochs,verbose=0, \n          validation_data=(x_test, y_test))","metadata":{"trusted":true,"_kg_hide-output":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from matplotlib import pyplot as plt\nplt.title('Accuracy')\nplt.plot(history.history['accuracy'], label='train')\nplt.plot(history.history['val_accuracy'], label='test')\nplt.ylim(.76, 1.01)\nplt.axhline(y=1.0, color=\"purple\", linestyle=\"--\")\nplt.legend()\nplt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}