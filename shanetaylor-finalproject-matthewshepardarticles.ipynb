{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Importing Pacakges, etc.","execution_count":null},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import nltk\nimport numpy as np\nimport pandas as pd\nimport pylab\nimport spacy\nimport matplotlib.pyplot  as plt\nimport multiprocessing as mp\nimport os\nimport gensim\nfrom cytoolz import *\nfrom sklearn.pipeline import *\nfrom sklearn.feature_extraction.text import *\nfrom sklearn.feature_extraction import *\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.decomposition import *\nfrom sklearn.cluster import *\nfrom sklearn.metrics import *\nfrom spacy import displacy\nfrom spacy.tokens import Token\nfrom spacy.lang.en.stop_words import STOP_WORDS\nfrom gensim.models import CoherenceModel, LdaModel, HdpModel, LsiModel\nfrom gensim.models.phrases import Phrases, Phraser\nfrom gensim.corpora import Dictionary\nimport pyLDAvis.gensim\npd.set_option('display.max_colwidth', 500)\nnlp = spacy.load('en', disable=['ner'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Importing Data","execution_count":null},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"df = pd.read_excel(\"../input/articles-final/articles_handcoded.xlsx\",header=0)\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Cleaning Text\nStriping non alpha-numeric characters, lowering case, moving strings of text into a list of tokens","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"nlp = spacy.load('en')\ndef tokenize(text):\n    return [tok.lower_ for tok in nlp.tokenizer(text) if (not tok.like_url) and (tok.is_alpha) and not (tok.is_stop)]\nwith mp.Pool() as p:\n    df['tokens'] = p.map(tokenize, df['Text'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Distribution of Processes\nThese refer to the type of activity or process occurring in each sentence","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## Which processes occur in each article?","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"(df['Process'].groupby(df['Source'])).value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def processmap(df):\n    df['material'] = df['Process'].map(lambda x: x == 'material')\n    df['mental'] = df['Process'].map(lambda x: x == 'mental')\n    df['verbal'] = df['Process'].map(lambda x: x == 'verbal')\n    df['attributive'] = df['Process'].map(lambda x: x == 'attributive')\n    df['existential'] = df['Process'].map(lambda x: x == 'existential')\nprocessmap(df)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"materials = df[df['material']==True].groupby(df['Source'])\nmentals = df[df['mental']==True].groupby(df['Source'])\nverbals = df[df['verbal']==True].groupby(df['Source'])\nattributives = df[df['attributive']==True].groupby(df['Source'])\nexistentials = df[df['existential']==True].groupby(df['Source'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df[['Source', 'material','mental','verbal','attributive',\n'existential']].pivot_table(columns=['Source'], aggfunc=np.sum)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df[['Source', 'material','mental','verbal','attributive',\n'existential']].pivot_table(columns=['Source'], aggfunc=np.sum).plot()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Nominalizations\nThese are the different words and phrases used to refer to the victim, Matthew Shepard, and the attackers, Henderson and McKinney","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## Nominalizations by article","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"(df['MS Nom'].groupby(df['Source'])).value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def sourcemap(df):\n    df['AP'] = df['Source'].map(lambda x: x == 'AP News')\n    df['BBC'] = df['Source'].map(lambda x: x == 'BBC')\n    df['Breitbart'] = df['Source'].map(lambda x: x == 'Breitbart')\n    df['CNN'] = df['Source'].map(lambda x: x == 'CNN')\n    df['Christian Courrier'] = df['Source'].map(lambda x: x == 'Christian Courrier')\n    df['Huffington Post'] = df['Source'].map(lambda x: x == 'Huffington Post')\n    df['NBC'] = df['Source'].map(lambda x: x == 'NBC')\n    df['New York Times'] = df['Source'].map(lambda x: x == 'New York Times')\n    df['The Guardian'] = df['Source'].map(lambda x: x == 'The Guardian')\n    df['Washington Examiner'] = df['Source'].map(lambda x: x == 'Washington Examiner')\nsourcemap(df)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ap = (df['MS Nom'].groupby(df['AP']==True)).value_counts()\nbbc = (df['MS Nom'].groupby(df['BBC']==True)).value_counts()\nbrei = (df['MS Nom'].groupby(df['Breitbart']==True)).value_counts()\ncnn = (df['MS Nom'].groupby(df['CNN']==True)).value_counts()\nchri = (df['MS Nom'].groupby(df['Christian Courrier']==True)).value_counts()\nhuff = (df['MS Nom'].groupby(df['Huffington Post']==True)).value_counts()\nnbc = (df['MS Nom'].groupby(df['NBC']==True)).value_counts()\nnyt = (df['MS Nom'].groupby(df['New York Times']==True)).value_counts()\nguar = (df['MS Nom'].groupby(df['The Guardian']==True)).value_counts()\nwash = (df['MS Nom'].groupby(df['Washington Examiner']==True)).value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.rc('font', size= 35)          \nplt.rc('axes', titlesize=15)    \nplt.rc('axes', labelsize=15)   \nplt.rc('xtick', labelsize=12)   \nplt.rc('ytick', labelsize=12)    \nplt.rc('figure', titlesize=40) \nplt.rcParams['figure.figsize'] = (25,25)\nfig, axes = plt.subplots(nrows = 5, ncols = 2)\nplt.subplots_adjust(hspace = .75, wspace = .5)\nplt.suptitle('Use of Nominal by Article')\n\nap.plot(ax = axes[0,0], kind = 'barh')\nplt.sca(axes[0,0])\nplt.title('Associated Press')\nplt.xlabel('Nominal')\nplt.xticks(rotation = 45)\nplt.ylabel('Use')\n\nbbc.plot(ax = axes[0,1], kind = 'barh')\nplt.sca(axes[0,1])\nplt.title('BBC')\nplt.xlabel('Nominal')\nplt.xticks(rotation = 45)\nplt.ylabel('Use')\n\nbrei.plot(ax = axes[1,0], kind = 'barh')\nplt.sca(axes[1,0])\nplt.title('Breitbart')\nplt.xlabel('Nominal')\nplt.xticks(rotation = 45)\nplt.ylabel('Use')\n\ncnn.plot(ax = axes[1,1], kind = 'barh')\nplt.sca(axes[1,1])\nplt.title(\"CNN\")\nplt.xlabel('Nominal')\nplt.xticks(rotation = 45)\nplt.ylabel('Use')\n\nchri.plot(ax = axes[2,0], kind = 'barh')\nplt.sca(axes[2,0])\nplt.title('Christian Courrier')\nplt.xlabel('Nominal')\nplt.xticks(rotation = 45)\nplt.ylabel('Use')\n\nhuff.plot(ax = axes[2,1], kind = 'barh')\nplt.sca(axes[2,1])\nplt.title('Huffington Post')\nplt.xlabel('Nominal')\nplt.xticks(rotation = 45)\nplt.ylabel('Use')\n\nnbc.plot(ax = axes[3,0], kind = 'barh')\nplt.sca(axes[3,0])\nplt.title('NBC')\nplt.xlabel('Nominal')\nplt.xticks(rotation = 45)\nplt.ylabel('Use')\n\nnyt.plot(ax = axes[3,1], kind = 'barh')\nplt.sca(axes[3,1])\nplt.title('New York Times')\nplt.xlabel('Nominal')\nplt.xticks(rotation = 45)\nplt.ylabel('Use')\n\nguar.plot(ax = axes[4,0], kind = 'barh')\nplt.sca(axes[4,0])\nplt.title(\"The Guardian\")\nplt.xlabel('Nominal')\nplt.xticks(rotation = 45)\nplt.ylabel('Use')\n\nwash.plot(ax = axes[4,1], kind = 'barh')\nplt.sca(axes[4,1])\nplt.title('Washington Examiner')\nplt.xlabel('Nominal')\nplt.xticks(rotation = 45)\nplt.ylabel('Use')\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"(df['HM Nom'].groupby(df['Source'])).value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ap_2 = (df['HM Nom'].groupby(df['AP']==True)).value_counts()\nbbc_2 = (df['HM Nom'].groupby(df['BBC']==True)).value_counts()\nbrei_2 = (df['HM Nom'].groupby(df['Breitbart']==True)).value_counts()\ncnn_2 = (df['HM Nom'].groupby(df['CNN']==True)).value_counts()\nchri_2 = (df['HM Nom'].groupby(df['Christian Courrier']==True)).value_counts()\nhuff_2 = (df['HM Nom'].groupby(df['Huffington Post']==True)).value_counts()\nnbc_2 = (df['HM Nom'].groupby(df['NBC']==True)).value_counts()\nnyt_2 = (df['HM Nom'].groupby(df['New York Times']==True)).value_counts()\nguar_2 = (df['HM Nom'].groupby(df['The Guardian']==True)).value_counts()\nwash_2 = (df['HM Nom'].groupby(df['Washington Examiner']==True)).value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.rc('font', size= 35)          \nplt.rc('axes', titlesize=15)    \nplt.rc('axes', labelsize=15)   \nplt.rc('xtick', labelsize=12)   \nplt.rc('ytick', labelsize=12)    \nplt.rc('figure', titlesize=40) \nplt.rcParams['figure.figsize'] = (25,25)\nfig, axes = plt.subplots(nrows = 5, ncols = 2)\nplt.subplots_adjust(hspace = .75, wspace = .5)\nplt.suptitle('Use of Nominal by Article')\n\nap_2.plot(ax = axes[0,0], kind = 'barh')\nplt.sca(axes[0,0])\nplt.title('Associated Press')\nplt.xlabel('Nominal')\nplt.xticks(rotation = 45)\nplt.ylabel('Use')\n\nbbc_2.plot(ax = axes[0,1], kind = 'barh')\nplt.sca(axes[0,1])\nplt.title('BBC')\nplt.xlabel('Nominal')\nplt.xticks(rotation = 45)\nplt.ylabel('Use')\n\nbrei_2.plot(ax = axes[1,0], kind = 'barh')\nplt.sca(axes[1,0])\nplt.title('Breitbart')\nplt.xlabel('Nominal')\nplt.xticks(rotation = 45)\nplt.ylabel('Use')\n\ncnn_2.plot(ax = axes[1,1], kind = 'barh')\nplt.sca(axes[1,1])\nplt.title(\"CNN\")\nplt.xlabel('Nominal')\nplt.xticks(rotation = 45)\nplt.ylabel('Use')\n\nchri_2.plot(ax = axes[2,0], kind = 'barh')\nplt.sca(axes[2,0])\nplt.title('Christian Courrier')\nplt.xlabel('Nominal')\nplt.xticks(rotation = 45)\nplt.ylabel('Use')\n\nhuff_2.plot(ax = axes[2,1], kind = 'barh')\nplt.sca(axes[2,1])\nplt.title('Huffington Post')\nplt.xlabel('Nominal')\nplt.xticks(rotation = 45)\nplt.ylabel('Use')\n\nnbc_2.plot(ax = axes[3,0], kind = 'barh')\nplt.sca(axes[3,0])\nplt.title('NBC')\nplt.xlabel('Nominal')\nplt.xticks(rotation = 45)\nplt.ylabel('Use')\n\nnyt_2.plot(ax = axes[3,1], kind = 'barh')\nplt.sca(axes[3,1])\nplt.title('New York Times')\nplt.xlabel('Nominal')\nplt.xticks(rotation = 45)\nplt.ylabel('Use')\n\nguar_2.plot(ax = axes[4,0], kind = 'barh')\nplt.sca(axes[4,0])\nplt.title(\"The Guardian\")\nplt.xlabel('Nominal')\nplt.xticks(rotation = 45)\nplt.ylabel('Use')\n\nwash_2.plot(ax = axes[4,1], kind = 'barh')\nplt.sca(axes[4,1])\nplt.title('Washington Examiner')\nplt.xlabel('Nominal')\nplt.xticks(rotation = 45)\nplt.ylabel('Use')\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Use of Passives\n### Number of Passives by Article","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"passives = (df['Passive'].groupby(df['Source'])).sum()\npassives.values","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Agentless Passives","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"empties = (df['Empty Passive'].groupby(df['Source'])).sum()\nempties.values","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Percentages","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"(empties/passives)*100","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Topic Modeling\n### First, I need to create the bigrams and then corpus I will use to generate the topics from the articles","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"texts = df['tokens']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"bigram = gensim.models.Phrases(texts)\ntexts = [bigram[line] for line in texts]\ndictionary = Dictionary(texts)\ncorpus= [dictionary.doc2bow(text) for text in texts]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### I will use both a Latent Semantic and Hierarchical Model","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"lsimodel = LsiModel(corpus = corpus, num_topics = 10, id2word = dictionary)\nlsimodel.show_topics(num_topics = 5) ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"hdpmodel = HdpModel(corpus = corpus, id2word = dictionary)\nhdpmodel.show_topics()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ldamodel = LdaModel(corpus = corpus, num_topics = 10, id2word = dictionary)\nldamodel.show_topics()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### And now, I can see what topics these models predict","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"pyLDAvis.enable_notebook()\npyLDAvis.gensim.prepare(ldamodel, corpus, dictionary)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Feature Clustering","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### First the text needs to be preprocessed","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"preprocessing = make_pipeline(TfidfVectorizer(analyzer=identity, min_df=3, max_df=0.3, norm='l2', use_idf=True), \n                    TfidfTransformer(norm='l2', use_idf=True)\n                    )\nX = preprocessing.fit_transform(df['tokens'])\nX.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\n\nclusterN=10\nwcss = []\nsiloutte_score =[]\nfor i in range(5, clusterN+1):\n    kmeans = KMeans(n_clusters = i, init = 'k-means++', max_iter=500, n_init=20, random_state = 0, n_jobs=4, precompute_distances=True)\n    kmeans.fit(X)\n    wcss.append(kmeans.inertia_)\n    siloutte_score.append(silhouette_score(X, kmeans.labels_))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Now I determine the number of clusters to use based on three tests","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.plot(range(4, clusterN+1), silhouette_score)\nplt.xticks(range(4, clusterN+1), range(4, clusterN+1))\nplt.title('The Silhouette Score plot')\nplt.xlabel('Number of clusters')\nplt.ylabel('silouette_scores')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.plot(range(4, clusterN+1), wcss)\nplt.xticks(range(4, clusterN+1), range(4, clusterN+1))\nplt.title('The Elbow Method')\nplt.xlabel('Number of clusters')\nplt.ylabel('WCSS')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cosines = -1 * np.ones(clusterN-2)\nfor i in range(len(wcss)-1):\n    if (wcss[i] < (wcss[i+1]+wcss[i-1])/2 ):\n       cosines[i]= (-1+(wcss[i-1]-wcss[i])*(wcss[i+1]-wcss[i]))/ \\\n       ((1+(wcss[i-1]-wcss[i])**2)*(1+ (wcss[i+1]-wcss[i])**2))**.5\n\nprint(np.flip(np.argsort(cosines))+5)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Based on these 3 tests, 8 appears to be a good number of clusters","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\n\nkmeans = KMeans(7, n_jobs=-1).fit(X)\ndf['cluster'] = kmeans.labels_\ndf.groupby('cluster')['Text'].count()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Keywords from Cluster\n### With this I can determine what concepts are indicative of each cluster","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def keywords(cluster, n=8):\n    f = pd.DataFrame({'all': pd.value_counts(list(concat(df['tokens'])))})\n    f['cl'] = pd.value_counts(list(concat(df[df['cluster']==cluster]['tokens'])))\n    f['pmi'] = np.log2( (f['cl'] * np.sum(f['all'])) / \n                        (f['all'] * np.sum(f['cl'])) )\n    return list(f['pmi'][f['all']>25].sort_values(ascending=False)[:n].index)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for i in range(8):\n    print(i,' '.join(keywords(i)))\ndist = kmeans.transform(X)\ndf['tokens'].iloc[dist[:,5].argsort()[:10]]","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}