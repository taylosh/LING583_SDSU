{"cells":[{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd\nfrom cytoolz import identity\nimport spacy\nfrom spacy.lang.en import English\nfrom spacy.lang.en.stop_words import STOP_WORDS\nfrom sklearn.metrics import confusion_matrix","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df = pd.read_csv(\"../input/classification.csv\")\ntest = pd.read_csv(\"../input/classification_test.csv\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Note that on kaggle.com I may not use `en_core_web_md` from SpaCy, but can `en`."},{"metadata":{"trusted":true},"cell_type":"code","source":"nlp = spacy.load('en', disable=['tagger', 'ner', 'parser'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Here I do not remove stop words and non-alphabic symbols."},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\n\ndef tokenize(text):\n    return [tok.orth_ for tok in nlp.tokenizer(text)]\ndf['tokens'] = df['text'].apply(tokenize)\n#def tokenize(text):\n#    return [tok.text for tok in nlp.tokenizer(text)]\n\ndf['tokens'] = df['text'].apply(tokenize)\ntest['tokens'] = test['text'].apply(tokenize)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let us create a martix with counts for every word in each document."},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.feature_extraction.text import *\ndtm = CountVectorizer(analyzer=identity)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X = dtm.fit_transform(df['tokens'])\nX_test = dtm.transform(test['tokens'])\nX.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":" So we have here 105489 columns. I got 93050 two weeks ago with the same code. I wonder if something was changed in Python modules after update. \n \nAnother lesson is found here: always run your code before presentation in a comfort of your home or office."},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.naive_bayes import *","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"modelNB = BernoulliNB()\nmodelNB.fit(X, df['sports'])\npredictions = modelNB.predict(X_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"I would like a function for error metrics computation, because I will compute different models on the test set  and on `df` data to compare results."},{"metadata":{"trusted":true},"cell_type":"code","source":"def error_metrics(model_name, true_values, predictions, errors=None):\n    if errors is None: \n        errors = pd.DataFrame({'metrics': ['accuracy', 'precision', 'recall', 'f1']})\n        \n    from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n    accuracy = accuracy_score(true_values, predictions)\n    precision = precision_score(true_values, predictions)\n    recall = recall_score(true_values, predictions)\n    f1 = f1_score(true_values, predictions)\n    errors[model_name]= [accuracy,  precision, recall, f1]\n    return errors","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let us save the errors for this model on both sets in a data frame. "},{"metadata":{"trusted":true},"cell_type":"code","source":"err = error_metrics(model_name='Test set with all columns', \n                    true_values = test['sports'], predictions=predictions)\nerr = error_metrics(model_name='Train set with all columns',\n                    true_values=df['sports'], predictions=modelNB.predict(X), errors=err)\nerr","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now I would like to see what happens with fewer columns. I will recycle my previous objects, so the script must be run as a whole."},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\n\ndef tokenize(text):\n    return [tok.text for tok in nlp.tokenizer(text.lower()) if (tok.text not in STOP_WORDS) and ((tok.text).isalpha())]\n\ndf['tokens'] = df['text'].apply(tokenize)\ntest['tokens'] = test['text'].apply(tokenize)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We are to create a new model object to train(fit)."},{"metadata":{"trusted":true},"cell_type":"code","source":"dtm = CountVectorizer(analyzer=identity)\nX = dtm.fit_transform(df['tokens'])\nX_test = dtm.transform(test['tokens'])\nX.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"modelNB = BernoulliNB()\nmodelNB.fit(X, df['sports'])\npredictions = modelNB.predict(X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"err = error_metrics(model_name='Test set with fewer columns', \n                    true_values = test['sports'], predictions=predictions, errors=err)\nerr = error_metrics(model_name='Train set with fewer columns',\n                          true_values=df['sports'], predictions=modelNB.predict(X), errors=err)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pd.set_option('display.max_columns', None)\nerr","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Usually our train set shows better results than our test set, because our model was trained on it.\n\n There could be 2 reasons why test metrics are better than train set metrics:\n\n1. This particular set happens to have less variance (noise) than our training set\n\n2. Our test set is less than our train set, and thus has less noise.\n\nComparing with model metrics computed with fewer columns we see that additional columns add noise for the model and interfere in model training, decreasing metrics.\n\nNote that when we have fewer columns we see better performance of the second model on a trainig set. This is so called **overfitting**, because the modell is better fitted to the test set. In particular, it is fitted to a random noise of the training set, and our test set might have a different random noise."},{"metadata":{},"cell_type":"markdown","source":"************************************************************************************************************************\nHere is a sound alert when your script finished running, where 500 is the frequency in Herz and 2000 is the duration in miliseconds. I found it here: https://stackoverflow.com/questions/16573051/sound-alarm-when-code-finishes"},{"metadata":{"trusted":true},"cell_type":"code","source":"import winsound\nwinsound.Beep(500, 2000)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.6"}},"nbformat":4,"nbformat_minor":4}